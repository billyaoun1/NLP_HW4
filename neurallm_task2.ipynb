{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b4dd3e-f5e5-4671-a86a-82372ac1b0fc",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927209b-31e2-4f54-a95a-fbaf7e5f00a3",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: __William Aoun__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25ecf4-4b08-4d35-b150-5db899662272",
   "metadata": {},
   "source": [
    "Task 2: Training your own word embeddings (15 points)\n",
    "--------------------------------\n",
    "\n",
    "For this task, you'll use the `gensim` package to train your own embeddings for both words and characters. These will eventually act as inputs to your neural language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89edeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n",
      "Requirement already satisfied: pip in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (65.5.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.1.1 setuptools-80.9.0 wheel-0.45.1\n",
      "Requirement already satisfied: nltk in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: gensim in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: torch in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchinfo in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# here are several dependencies to install\n",
    "!python --version\n",
    "!python -m pip install --upgrade pip setuptools wheel\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install torch torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd110e8-9404-4bf3-b6c2-27120adfd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/billyaoun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/billyaoun/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils_starter as nutils\n",
    "\n",
    "# for word embeddings\n",
    "# if not installed, run the following command:\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278aa8e0-2dce-4d33-8312-4b2b0146dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on google colab, you'll need to mount your drive to access data files\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ce4aff-bba6-429d-b618-c57c23b350fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "\n",
    "# The dimensions of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# DO NOT WRITE \"50\" WHEN YOU ARE REFERRING TO THE EMBEDDING SIZE\n",
    "EMBEDDINGS_SIZE = 50\n",
    "\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "NGRAM = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb948-4d52-41b4-a3e6-4b77d4232032",
   "metadata": {},
   "source": [
    "Train embeddings on provided dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d77d3-1d87-4cd4-a9b6-acbed5d2c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word format:\n",
      "Sentence 1: ['<s>', '<s>', 'this', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'i', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'i', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.', '</s>', '</s>']\n",
      "Sentence 2: ['<s>', '<s>', 'it', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.', '</s>', '</s>']\n",
      "\n",
      "char format:\n",
      "sentence 1:\n",
      "  ['<s>', '<s>', 't', 'h', 'i', 's', '_', 'p', 'r', 'o', 'c', 'e', 's', 's', ',', '_', 'h', 'o', 'w', 'e']\n",
      "  ['v', 'e', 'r', ',', '_', 'a', 'f', 'f', 'o', 'r', 'd', 'e', 'd', '_', 'm', 'e', '_', 'n', 'o', '_']\n",
      "  ['m', 'e', 'a', 'n', 's', '_', 'o', 'f', '_', 'a', 's', 'c', 'e', 'r', 't', 'a', 'i', 'n', 'i', 'n']\n",
      "  ['g', '_', 't', 'h', 'e', '_', 'd', 'i', 'm', 'e', 'n', 's', 'i', 'o', 'n', 's', '_', 'o', 'f', '_']\n",
      "  ['m', 'y', '_', 'd', 'u', 'n', 'g', 'e', 'o', 'n', ';', '_', 'a', 's', '_', 'i', '_', 'm', 'i', 'g']\n",
      "  ['h', 't', '_', 'm', 'a', 'k', 'e', '_', 'i', 't', 's', '_', 'c', 'i', 'r', 'c', 'u', 'i', 't', ',']\n",
      "  ['_', 'a', 'n', 'd', '_', 'r', 'e', 't', 'u', 'r', 'n', '_', 't', 'o', '_', 't', 'h', 'e', '_', 'p']\n",
      "  ['o', 'i', 'n', 't', '_', 'w', 'h', 'e', 'n', 'c', 'e', '_', 'i', '_', 's', 'e', 't', '_', 'o', 'u']\n",
      "  ['t', ',', '_', 'w', 'i', 't', 'h', 'o', 'u', 't', '_', 'b', 'e', 'i', 'n', 'g', '_', 'a', 'w', 'a']\n",
      "  ['r', 'e', '_', 'o', 'f', '_', 't', 'h', 'e', '_', 'f', 'a', 'c', 't', ';', '_', 's', 'o', '_', 'p']\n",
      "  ['e', 'r', 'f', 'e', 'c', 't', 'l', 'y', '_', 'u', 'n', 'i', 'f', 'o', 'r', 'm', '_', 's', 'e', 'e']\n",
      "  ['m', 'e', 'd', '_', 't', 'h', 'e', '_', 'w', 'a', 'l', 'l', '.', '</s>', '</s>']\n",
      "sentence 2:\n",
      "  ['<s>', '<s>', 'i', 't', '_', 'n', 'e', 'v', 'e', 'r', '_', 'o', 'n', 'c', 'e', '_', 'o', 'c', 'c', 'u']\n",
      "  ['r', 'r', 'e', 'd', '_', 't', 'o', '_', 'm', 'e', '_', 't', 'h', 'a', 't', '_', 't', 'h', 'e', '_']\n",
      "  ['f', 'u', 'm', 'b', 'l', 'i', 'n', 'g', '_', 'm', 'i', 'g', 'h', 't', '_', 'b', 'e', '_', 'a', '_']\n",
      "  ['m', 'e', 'r', 'e', '_', 'm', 'i', 's', 't', 'a', 'k', 'e', '.', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# use the provided utility functions to read in the data\n",
    "\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "\n",
    "# read the spooky data in both by character and by word using the read_file_spooky function in the\n",
    "# provided utils\n",
    "word_data = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM, by_character=False)\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM, by_character=True)\n",
    "\n",
    "# print out the first two sentences in each format\n",
    "# make sure we can read the output easily without scrolling to the side too much\n",
    "\n",
    "print(\"word format:\")\n",
    "for i in range(2):\n",
    "    print(f\"Sentence {i+1}: {word_data[i]}\")\n",
    "\n",
    "print(\"\\nchar format:\")\n",
    "for i in range(2):\n",
    "    tokens = char_data[i]\n",
    "    chunks = [tokens[j : j + 20] for j in range(0, len(tokens), 20)]\n",
    "    print(f\"sentence {i+1}:\")\n",
    "    for chunk in chunks:\n",
    "        print(f\"  {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704033c",
   "metadata": {},
   "source": [
    "8. What character represents spaces when we tokenize by character? __underscore__\n",
    "9. Read the word2vec documentation. What do the following parameters signify?\n",
    "    - embeddings_size: __The dimensionality of the dense vector representation for each word__\n",
    "    - window: __The max distance between the current and predicted word within a sentence__\n",
    "    - min_count: __Ignores all words with total frequency lower that this value__\n",
    "    - sg: __Represents skip-gram when sg=1 and CBOW when sg=0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33920ce1-faee-4774-8aae-d78e511bf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "# create your word embeddings\n",
    "# use the skip gram algorithm and a window size of 5\n",
    "# min_count should be 1\n",
    "# takes ~3.3 sec on Felix's computer for character embeddings using skip-gram with window size 5\n",
    "# takes ~3.3 sec on Felix's computer for word embeddings using skip-gram with window size 5 \n",
    "\n",
    "\n",
    "def train_word2vec(data: list[list[str]], embeddings_size: int,\n",
    "                    window: int = 5, min_count: int = 1, sg: int = 1) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Create new word embeddings based on our data.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A gensim Word2Vec model\n",
    "        https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    \"\"\"\n",
    "    model = Word2Vec(data, vector_size=embeddings_size, window=window, \n",
    "                    min_count=min_count, sg=sg)\n",
    "    return model\n",
    "\n",
    "\n",
    "# After you are happy with this function, copy + paste it into the bottom of \n",
    "# your neurallm_utils.py file\n",
    "# You'll need it for the next task!\n",
    "def create_embedder(raw_embeddings: Word2Vec) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    Create a PyTorch embedding layer based on our data.\n",
    "\n",
    "    We will *first* train a Word2Vec model on our data.\n",
    "    Then, we'll use these weights to create a PyTorch embedding layer.\n",
    "        `nn.Embedding.from_pretrained(weights)`\n",
    "\n",
    "\n",
    "    PyTorch docs: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained\n",
    "    Gensim Word2Vec docs: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "    Pay particular attention to the *types* of the weights and the types required by PyTorch.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint:\n",
    "    # For later tasks, we'll need two mappings: One from token to index, and one from index to tokens.\n",
    "    # It might be a good idea to store these as properties of your embedder.\n",
    "    # e.g. `embedder.token_to_index = ...`\n",
    "    \n",
    "    weights = torch.FloatTensor(raw_embeddings.wv.vectors)\n",
    "   \n",
    "    embedder = torch.nn.Embedding.from_pretrained(weights)\n",
    "    \n",
    "    embedder.token_to_index = {token: idx for idx, token in enumerate(raw_embeddings.wv.index_to_key)}\n",
    "    embedder.index_to_token = {idx: token for token, idx in embedder.token_to_index.items()}\n",
    "    \n",
    "    return embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db8e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save both sets (word and character based) of Word2Vec embeddings.\n",
    "# Use the provided utility functions in nutils.\n",
    "# These will be (re)loaded in the next notebook.\n",
    "word_data = nutils.read_file_spooky(TRAIN_FILE, ngram=1, by_character=False)\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, ngram=1, by_character=True)\n",
    "\n",
    "word_embeddings = train_word2vec(word_data, EMBEDDINGS_SIZE)\n",
    "char_embeddings = train_word2vec(char_data, EMBEDDINGS_SIZE)\n",
    "\n",
    "nutils.save_word2vec(word_embeddings, EMBEDDING_SAVE_FILE_WORD)\n",
    "nutils.save_word2vec(char_embeddings, EMBEDDING_SAVE_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c85728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load them in again to make sure that this works and is still fast\n",
    "word_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "char_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57301fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the embedders\n",
    "word_embedder = create_embedder(word_embeddings)\n",
    "char_embedder = create_embedder(char_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d7c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedder mappings:\n",
      "token_to_index type: <class 'dict'>\n",
      "index_to_token type: <class 'dict'>\n",
      "token_to_index: [(',', 0), ('the', 1), ('of', 2), ('<s>', 3), ('</s>', 4)]\n",
      "index_to_token: [(0, ','), (1, 'the'), (2, 'of'), (3, '<s>'), (4, '</s>')]\n",
      "\n",
      "Char embedder mappings:\n",
      "token_to_index type: <class 'dict'>\n",
      "index_to_token type: <class 'dict'>\n",
      "token_to_index: [('_', 0), ('e', 1), ('t', 2), ('a', 3), ('o', 4)]\n",
      "index_to_token: [(0, '_'), (1, 'e'), (2, 't'), (3, 'a'), (4, 'o')]\n"
     ]
    }
   ],
   "source": [
    "# take a look at your saved token to index and index to token mappings in your embedders to make sure they make sense\n",
    "# AND that they are both dictionaries mapping from int to str or vice versa!\n",
    "# don't leave a ton of output in your notebook when you turn it in, but you need to understand this,\n",
    "# and it's an easy place to make a mistake that's hard to debug later.\n",
    "# do leave whatever code you use here, comment it out if it produces a lot of output\n",
    "\n",
    "print(\"Word embedder mappings:\")\n",
    "print(f\"token_to_index type: {type(word_embedder.token_to_index)}\")\n",
    "print(f\"index_to_token type: {type(word_embedder.index_to_token)}\")\n",
    "print(f\"token_to_index: {list(word_embedder.token_to_index.items())[:5]}\")\n",
    "print(f\"index_to_token: {list(word_embedder.index_to_token.items())[:5]}\")\n",
    "\n",
    "print(\"\\nChar embedder mappings:\")\n",
    "print(f\"token_to_index type: {type(char_embedder.token_to_index)}\")\n",
    "print(f\"index_to_token type: {type(char_embedder.index_to_token)}\")\n",
    "print(f\"token_to_index: {list(char_embedder.token_to_index.items())[:5]}\")\n",
    "print(f\"index_to_token: {list(char_embedder.index_to_token.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb66cea2-746e-4371-8966-33dd358d46c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings vocab size: 25374\n",
      "Char embeddings vocab size: 60\n"
     ]
    }
   ],
   "source": [
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n",
    "print(f\"Word embeddings vocab size: {len(word_embedder.token_to_index)}\")\n",
    "print(f\"Char embeddings vocab size: {len(char_embedder.token_to_index)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv311)",
   "language": "python",
   "name": "myenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
