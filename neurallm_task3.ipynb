{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8dcfc9-0378-4629-8f9f-58e64f121c3b",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634622b-b20d-43a3-a3f0-d6adc6ab64fe",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: __William Aoun__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47258c1-5462-4f83-afc5-2badaeb4c33d",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (80 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f24a57-464a-4194-9fee-36411939e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t7/lfwjzzyx18d597y_qkgdq6zh0000gn/T/ipykernel_12353/3214850165.py:6: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/billyaoun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/billyaoun/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils_starter as nutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# This function gives us nice print-outs of our models.\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff97dc8-e587-46c6-8eff-69b9c41b6099",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit constants as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "OUTPUT_WORDS = 'generated_wordbased.txt' # The file to save your generated sentences for word-based LM\n",
    "OUTPUT_CHARS = 'generated_charbased.txt' # The file to save your generated sentences for char-based LM\n",
    "\n",
    "# you can update these file names if you want to depending on how you are exploring \n",
    "# hyperparameters\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "MODEL_FILE_WORD = f'spooky_author_model_word_{NGRAM}.pt' # The file to save your trained word-based neural LM to\n",
    "MODEL_FILE_CHAR = f'spooky_author_model_char_{NGRAM}.pt' # The file to save your trained char-based neural LM to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6669b3-3df5-4d80-b67c-7c8c5f5fd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your word vectors that you made in your previous notebook AND\n",
    "# use the create_embedder function to make your pytorch embedder\n",
    "word_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "char_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)\n",
    "\n",
    "word_embedder = nutils.create_embedder(word_embeddings)\n",
    "char_embedder = nutils.create_embedder(char_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b664f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll also need to re-load your text data\n",
    "word_data = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM, by_character=False)\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, ngram=NGRAM, by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578103c1-6388-4f3b-abbc-54e459c1ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to vectorize a text corpus.\n",
    "# Here, it creates a mapping from word to that word's unique index.\n",
    "\n",
    "# Hint: use one of the dicts from your embedding function.\n",
    "\n",
    "def encode_tokens(data: list[list[str]], embedder: torch.nn.Embedding) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "    for sentence in data:\n",
    "        encoded_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in embedder.token_to_index:\n",
    "                encoded_sentence.append(embedder.token_to_index[token])\n",
    "            else:\n",
    "                encoded_sentence.append(0)\n",
    "        encoded.append(encoded_sentence)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561ad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode your data from tokens to integers for both word and char embeddings\n",
    "word_encoded = encode_tokens(word_data, word_embedder)\n",
    "char_encoded = encode_tokens(char_data, char_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2296f18f-8f8f-41a7-85dc-b3661f12feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedder vocab size: 25374\n",
      "Char embedder vocab size: 60\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the mappings for each of your embedders.\n",
    "# these should match the vocab sizes you calculated in Task 2\n",
    "print(f\"Word embedder vocab size: {len(word_embedder.token_to_index)}\")\n",
    "print(f\"Char embedder vocab size: {len(char_embedder.token_to_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbd01e-d778-4542-8bb9-9086cdf4c06e",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556a723-7cfd-4f86-a424-0ba2e15acfb5",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences\n",
    "\n",
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (y).\n",
    "\n",
    "Example: this process however afforded me\n",
    "\n",
    "Would become:\n",
    "```\n",
    "X\n",
    "[[this,    process]\n",
    "[process, however]\n",
    "[however, afforded]]\n",
    "\n",
    "y\n",
    "[however,\n",
    "afforded,\n",
    "me]\n",
    "```\n",
    "\n",
    "\n",
    "Our first step is to generate n-grams like we have always been doing. We'll just do this \n",
    "on our encoded data instead of the raw text. (Feel free to consult your past HW here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e0fc28-e667-4d72-9be2-afd749cb9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "    samples = []\n",
    "    for sentence in encoded:\n",
    "        for i in range(len(sentence) - ngram + 1):\n",
    "            sample = sentence[i:i + ngram]\n",
    "            samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d80353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word sequences: 634080\n",
      "First 5 word samples:\n",
      "[3, 3, 31]\n",
      "[3, 31, 2959]\n",
      "[31, 2959, 0]\n",
      "[2959, 0, 154]\n",
      "[0, 154, 0]\n",
      "\n",
      "Char sequences: 2957553\n",
      "First 5 char samples:\n",
      "[25, 25, 2]\n",
      "[25, 2, 8]\n",
      "[2, 8, 6]\n",
      "[8, 6, 7]\n",
      "[6, 7, 0]\n",
      "\n",
      "Token 0 maps to word: ','\n",
      "Token 0 maps to char: '_'\n"
     ]
    }
   ],
   "source": [
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [0, 0, 31]\n",
    "# [0, 31, 2959]\n",
    "# [31, 2959, 2]\n",
    "# ...\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [20, 20, 2]\n",
    "# [20, 2, 8]\n",
    "# [2, 8, 6]\n",
    "# ...\n",
    "\n",
    "# print out the first 5 training samples for each and make sure that the\n",
    "# windows are sliding one word at a time. These should be integers!\n",
    "# make sure that they map to the correct words in your vocab\n",
    "# Hint: what word maps to token 0?\n",
    "\n",
    "word_samples = generate_ngram_training_samples(word_encoded, NGRAM)\n",
    "char_samples = generate_ngram_training_samples(char_encoded, NGRAM)\n",
    "\n",
    "print(f\"Word sequences: {len(word_samples)}\")\n",
    "print(\"First 5 word samples:\")\n",
    "for i in range(5):\n",
    "    print(word_samples[i])\n",
    "\n",
    "print(f\"\\nChar sequences: {len(char_samples)}\")\n",
    "print(\"First 5 char samples:\")\n",
    "for i in range(5):\n",
    "    print(char_samples[i])\n",
    "\n",
    "print(f\"\\nToken 0 maps to word: '{word_embedder.index_to_token[0]}'\")\n",
    "print(f\"Token 0 maps to char: '{char_embedder.index_to_token[0]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb275f6c-bff7-465f-b4c3-759610d44113",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a DataLoader (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531ecf47-13ad-403d-a5e2-e19d462aab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word X shape: 634080 sequences, 2 elements each\n",
      "Word y shape: 634080 elements\n",
      "Char X shape: 2957553 sequences, 2 elements each\n",
      "Char y shape: 2957553 elements\n"
     ]
    }
   ],
   "source": [
    "# Note here that each sequence we've created so far is in the form:\n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate them into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here for both word and character data\n",
    "# you can write a function to do this if you'd like (not required, might be helpful)\n",
    "\n",
    "\n",
    "# print out the shapes (or lengths to know how many sequences there are and how many\n",
    "# elements each sub-list has) for word-based to verify that they are correct\n",
    "\n",
    "# print out the shapes for char-based to verify that they are correct\n",
    "\n",
    "\n",
    "def split_xy(samples: list) -> tuple:\n",
    "    X = [sample[:-1] for sample in samples]\n",
    "    y = [sample[-1] for sample in samples]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "word_X, word_y = split_xy(word_samples)\n",
    "char_X, char_y = split_xy(char_samples)\n",
    "\n",
    "print(f\"Word X shape: {len(word_X)} sequences, {len(word_X[0])} elements each\")\n",
    "print(f\"Word y shape: {len(word_y)} elements\")\n",
    "\n",
    "print(f\"Char X shape: {len(char_X)} sequences, {len(char_X[0])} elements each\")\n",
    "print(f\"Char y shape: {len(char_y)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b099f7f6-be95-49f2-89e6-fa7833fcb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    total_size = len(X)\n",
    "    test_size = int(total_size * test_pct)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    train_dataset = TensorDataset(X_tensor[:train_size], y_tensor[:train_size])\n",
    "    test_dataset = TensorDataset(X_tensor[train_size:], y_tensor[train_size:])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=num_sequences_per_batch, shuffle=shuffle\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=num_sequences_per_batch, shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ef8b",
   "metadata": {},
   "source": [
    "### some definitions:\n",
    "- a single __batch__ is the number of sequences that your model will evaluate at once when it learns\n",
    "-  __steps per epoch__ is the number of batches that your model will see in a single epoch  (one pass through the data)-- your NUM_SEQUENCES_PER_BATCH constant is the batch size--you won't need this for pytorch but it's useful to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f96706-8a23-4ca7-82ab-35d9f47a1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD DATA:\n",
      "Train X shape: torch.Size([128, 2])\n",
      "Train y shape: torch.Size([128])\n",
      "Test X shape: torch.Size([128, 2])\n",
      "Test y shape: torch.Size([128])\n",
      "\n",
      "CHAR DATA:\n",
      "Train X shape: torch.Size([128, 2])\n",
      "Train y shape: torch.Size([128])\n",
      "Test X shape: torch.Size([128, 2])\n",
      "Test y shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# initialize your dataloaders for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is\n",
    "# correct for both word and character data\n",
    "# note that your train data and your test data should have the same shapes!\n",
    "# print enough information to verify that the shapes are correct\n",
    "\n",
    "\n",
    "# Examples:\n",
    "# Normally you would loop over your dataloader, but we just want to get a single batch to test it out:\n",
    "# Every time you call next, you advance to the next batch\n",
    "# sample_X, sample_y = next(iter(train_dataloader))\n",
    "# sample_X.shape # (batch_size, n-1)\n",
    "# sample_y.shape  # (batch_size)\n",
    "\n",
    "word_train_loader, word_test_loader = create_dataloaders(\n",
    "    word_X, word_y, NUM_SEQUENCES_PER_BATCH\n",
    ")\n",
    "char_train_loader, char_test_loader = create_dataloaders(\n",
    "    char_X, char_y, NUM_SEQUENCES_PER_BATCH\n",
    ")\n",
    "\n",
    "print(\"WORD DATA:\")\n",
    "word_sample_X, word_sample_y = next(iter(word_train_loader))\n",
    "print(f\"Train X shape: {word_sample_X.shape}\")\n",
    "print(f\"Train y shape: {word_sample_y.shape}\")\n",
    "\n",
    "word_test_X, word_test_y = next(iter(word_test_loader))\n",
    "print(f\"Test X shape: {word_test_X.shape}\")\n",
    "print(f\"Test y shape: {word_test_y.shape}\")\n",
    "\n",
    "print(\"\\nCHAR DATA:\")\n",
    "char_sample_X, char_sample_y = next(iter(char_train_loader))\n",
    "print(f\"Train X shape: {char_sample_X.shape}\")\n",
    "print(f\"Train y shape: {char_sample_y.shape}\")\n",
    "\n",
    "char_test_X, char_test_y = next(iter(char_test_loader))\n",
    "print(f\"Test X shape: {char_test_X.shape}\")\n",
    "print(f\"Test y shape: {char_test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207dc63-8e68-47f1-b3f7-daf0c0d855f9",
   "metadata": {},
   "source": [
    "### d) Define, train & save your models (25 points)\n",
    "\n",
    "Write the code to train feedforward neural language models for both word embeddings and character embeddings make sure not to just copy + paste to train your two models (define functions as needed).\n",
    "\n",
    "Define your model architecture using PyTorch layers and activation functions. When training, use the Adam optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) instead of sgd (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD).\n",
    "\n",
    "add cells as desired :)\n",
    "\n",
    "Your FFNN should have the following architecture:\n",
    "- It should be a two layer neural net (one hidden layer, one output layer)\n",
    "- It should use ReLU as its activation function\n",
    "\n",
    "Our biggest piece of advice--make sure that you understand what dimensions each layer needs to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac10d29e-5af0-46b8-a2e6-a96832b7af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing our implementation of a Feed-Forward Neural Network.\n",
    "    You will need to implement two methods:\n",
    "        - A constructor to set up the architecture and hyperparameters of the model\n",
    "        - The forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model. \n",
    "        \n",
    "        You can change these parameters as you would like.\n",
    "        Once you get a working model, you are encouraged to\n",
    "        experiment with this constructor to improve performance.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: The number of words in the vocabulary\n",
    "            ngram: The value of N for training and prediction.\n",
    "            embedding_layer: The previously trained embedder. \n",
    "            hidden_units: The size of the hidden layer.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # we recommend saving the parameters as instance variables\n",
    "        # so you can access them later as needed\n",
    "        # (in addition to anything else you need to do here)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram = ngram\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.embedding = embedding_layer\n",
    "        \n",
    "        embedding_dim = embedding_layer.weight.shape[1]\n",
    "        input_size = (ngram - 1) * embedding_dim\n",
    "        \n",
    "        self.hidden = nn.Linear(input_size, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.output = nn.Linear(hidden_units, vocab_size)\n",
    "    \n",
    "    def forward(self, X: list) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the network.\n",
    "        This is not a prediction, and it should not apply softmax.\n",
    "\n",
    "        Params:\n",
    "            X: the input data\n",
    "\n",
    "        Returns:\n",
    "            The output of the model; i.e. its predictions.\n",
    "        \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(X)  \n",
    "       \n",
    "        flattened = embedded.view(embedded.size(0), -1) \n",
    "        \n",
    "        hidden_out = self.relu(self.hidden(flattened))\n",
    "        \n",
    "        output = self.output(hidden_out)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66b6a91-2c8b-4072-815d-fd83b0a6fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Our model's training loop.\n",
    "    Print the cross entropy loss every epoch.\n",
    "    You should use the Adam optimizer instead of SGD.\n",
    "\n",
    "    When looking for documentation, try to stay on PyTorch's website.\n",
    "    This might be a good place to start: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html \n",
    "    They should have plenty of tutorials, and we don't want you to get confused from other resources.\n",
    "\n",
    "    Params:\n",
    "        dataloader: The training dataloader\n",
    "        model: The model we wish to train\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: Learning rate \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # you will need to initialize an optimizer and a loss function, which you should do\n",
    "    # before the training loop\n",
    "\n",
    "    # print out the epoch number and the current average loss after each epoch\n",
    "    # you can use tqdm to print out a progress bar\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b981e7",
   "metadata": {},
   "source": [
    "For the next part, we're testing our model's functions so we can see if it works.\n",
    "No need to do this on both the word and character data, just one is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0024e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FFNN                                     [128, 25374]              --\n",
       "â”œâ”€Embedding: 1-1                         [128, 2, 50]              (1,268,700)\n",
       "â”œâ”€Linear: 1-2                            [128, 128]                12,928\n",
       "â”œâ”€ReLU: 1-3                              [128, 128]                --\n",
       "â”œâ”€Linear: 1-4                            [128, 25374]              3,273,246\n",
       "==========================================================================================\n",
       "Total params: 4,554,874\n",
       "Trainable params: 3,286,174\n",
       "Non-trainable params: 1,268,700\n",
       "Total mult-adds (Units.MEGABYTES): 583.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 26.22\n",
       "Params size (MB): 18.22\n",
       "Estimated Total Size (MB): 44.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "# Print out its architecture (use the imported summary function)\n",
    "word_model = FFNN(len(word_embedder.token_to_index), NGRAM, word_embedder)\n",
    "print(\"Word Model Architecture:\")\n",
    "summary(\n",
    "    word_model, input_size=(NUM_SEQUENCES_PER_BATCH, NGRAM - 1), dtypes=[torch.long]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf2d1d4-c860-46b6-a44e-c1627a2d77f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [01:56<00:00, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 5.7668\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# train your models for 1 epoch\n",
    "# see timing information posted on Canvas!\n",
    "\n",
    "# re-create your data loader fresh\n",
    "word_train_loader, word_test_loader = create_dataloaders(\n",
    "    word_X, word_y, NUM_SEQUENCES_PER_BATCH\n",
    ")\n",
    "# train your model\n",
    "print(\"Training word model...\")\n",
    "train(word_train_loader, word_model, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755443f",
   "metadata": {},
   "source": [
    "10. You're reporting the loss after each epoch of training. What is the loss for your model after 1 epoch?\n",
    "- word or character-based? __word__\n",
    "- loss? __5.7668__\n",
    "\n",
    "Loss isn't accuracy, but it does tell us whether or not the model is improving over time. For character-based, loss after one epoch should be ~2.1; for word-based it is ~5.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31e2d",
   "metadata": {},
   "source": [
    "### e) create a full pipeline (13 points)\n",
    "\n",
    "We've made all the pieces that you'll need for a full pipeline, now let's package everything together nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b22dfd78-fc75-41eb-ba67-17b1141ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "# make a function that does your full *training* pipeline\n",
    "# This is essentially pulling the pieces that you've done so far earlier in this \n",
    "# notebook into a single function that you can call to train your model\n",
    "\n",
    "\n",
    "def full_pipeline(data: list[list[str]], word_embeddings_filename: str, \n",
    "                batch_size:int = NUM_SEQUENCES_PER_BATCH,\n",
    "                ngram:int = NGRAM, hidden_units = 128, epochs = 1,\n",
    "                lr = 0.001, test_pct = 0.1,\n",
    "                ) -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from loading embeddings to training.\n",
    "    You won't use the test set for anything.\n",
    "\n",
    "    Params:\n",
    "        data: The raw data to train on, parsed as a list of lists of tokens\n",
    "        word_embeddings_filename: The filename of the Word2Vec word embeddings\n",
    "        batch_size: The batch size to use\n",
    "        hidden_units: The number of hidden units to use\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: The learning rate to use\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    embeddings = nutils.load_word2vec(word_embeddings_filename)\n",
    "    embedder = nutils.create_embedder(embeddings)\n",
    "    \n",
    "    encoded_data = encode_tokens(data, embedder)\n",
    "    samples = generate_ngram_training_samples(encoded_data, ngram)\n",
    "    X, y = split_xy(samples)\n",
    "    train_loader, test_loader = create_dataloaders(X, y, batch_size, test_pct)\n",
    "    model = FFNN(len(embedder.token_to_index), ngram, embedder, hidden_units)\n",
    "    train(train_loader, model, epochs, lr)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24546581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE TRAINING (1 epoch) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:07<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 5.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:26<00:00, 783.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 2.0855\n",
      "\n",
      "=== EXPLORING EPOCHS ===\n",
      "Training word model with 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:05<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 5.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:02<00:00, 36.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 5.2258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:03<00:00, 36.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 4.9900\n",
      "Training char model with 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:20<00:00, 1031.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 2.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:21<00:00, 961.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 1.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:19<00:00, 1059.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 1.9685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:22<00:00, 919.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 1.9611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:18<00:00, 1104.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 1.9567\n",
      "\n",
      "=== EXPLORING LEARNING RATES ===\n",
      "Training word model with lr=0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [01:38<00:00, 45.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Average Loss: 5.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [01:27<00:00, 50.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Average Loss: 5.5635\n",
      "Training char model with lr=0.0001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:18<00:00, 1096.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Average Loss: 2.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:18<00:00, 1129.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Average Loss: 2.1688\n",
      "\n",
      "=== EXPLORING HIDDEN UNITS ===\n",
      "Training word model with 256 hidden units...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:20<00:00, 31.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Average Loss: 5.7135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4459/4459 [02:17<00:00, 32.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Average Loss: 5.1393\n",
      "Training char model with 64 hidden units...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:17<00:00, 1159.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Average Loss: 2.1310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20796/20796 [00:17<00:00, 1186.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Average Loss: 2.0160\n"
     ]
    }
   ],
   "source": [
    "# 10 points\n",
    "\n",
    "# Use your full pipeline to train models on the word data and the character data.\n",
    "# Feel free to add cells if you'd like to.\n",
    "\n",
    "# Train your models however you'd like. Play around with number of epochs, learning rate, etc.\n",
    "# Do whatever you'd like to for exploring hyperparameters.\n",
    "# You aren't required to hit a certain loss, but you should leave code here that shows\n",
    "# that you explored effects of changing at least two of the different hyperparameters\n",
    "# Please don't change the architecture of the model (keep it a 2-layer model with 1 hidden layer)\n",
    "\n",
    "# You'll likely want to do this exploration AFTER completing your prediction and generation code, so start\n",
    "# with just training for 1 - 5 epochs with default params.\n",
    "\n",
    "\n",
    "# Word-based takes Felix's computer 7 - 8 min for 5 epochs with default params running on CPU\n",
    "# Char-based Felix's computer ~1min 30sec - 2min for 5 epochs with default params running on CPU\n",
    "\n",
    "# Start with default parameters - 1 epoch\n",
    "print(\"=== BASELINE TRAINING (1 epoch) ===\")\n",
    "word_model_1ep = full_pipeline(word_data, EMBEDDING_SAVE_FILE_WORD, epochs=1)\n",
    "char_model_1ep = full_pipeline(char_data, EMBEDDING_SAVE_FILE_CHAR, epochs=1)\n",
    "\n",
    "# Hyperparameter exploration 1: Different number of epochs\n",
    "print(\"\\n=== EXPLORING EPOCHS ===\")\n",
    "print(\"Training word model with 3 epochs...\")\n",
    "word_model_3ep = full_pipeline(word_data, EMBEDDING_SAVE_FILE_WORD, epochs=3)\n",
    "\n",
    "print(\"Training char model with 5 epochs...\")\n",
    "char_model_5ep = full_pipeline(char_data, EMBEDDING_SAVE_FILE_CHAR, epochs=5)\n",
    "\n",
    "# Hyperparameter exploration 2: Different learning rates\n",
    "print(\"\\n=== EXPLORING LEARNING RATES ===\")\n",
    "print(\"Training word model with lr=0.01...\")\n",
    "word_model_high_lr = full_pipeline(\n",
    "    word_data, EMBEDDING_SAVE_FILE_WORD, epochs=2, lr=0.01\n",
    ")\n",
    "\n",
    "print(\"Training char model with lr=0.0001...\")\n",
    "char_model_low_lr = full_pipeline(\n",
    "    char_data, EMBEDDING_SAVE_FILE_CHAR, epochs=2, lr=0.0001\n",
    ")\n",
    "\n",
    "# Hyperparameter exploration 3: Different hidden units\n",
    "print(\"\\n=== EXPLORING HIDDEN UNITS ===\")\n",
    "print(\"Training word model with 256 hidden units...\")\n",
    "word_model_big = full_pipeline(\n",
    "    word_data, EMBEDDING_SAVE_FILE_WORD, epochs=2, hidden_units=256\n",
    ")\n",
    "\n",
    "print(\"Training char model with 64 hidden units...\")\n",
    "char_model_small = full_pipeline(\n",
    "    char_data, EMBEDDING_SAVE_FILE_CHAR, epochs=2, hidden_units=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1171c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to spooky_author_model_word_3.pt and spooky_author_model_char_3.pt\n"
     ]
    }
   ],
   "source": [
    "# when you're happy with them, save both models\n",
    "# Feel free to play around with any hyperparameters you'd like\n",
    "\n",
    "# using torch.save and the model's state_dict\n",
    "# torch.save(word_model.state_dict(), MODEL_FILE_WORD)\n",
    "# torch.save(char_model.state_dict(), MODEL_FILE_CHAR)\n",
    "\n",
    "torch.save(word_model_3ep.state_dict(), MODEL_FILE_WORD)\n",
    "torch.save(char_model_5ep.state_dict(), MODEL_FILE_CHAR)\n",
    "print(f\"Models saved to {MODEL_FILE_WORD} and {MODEL_FILE_CHAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e554fe9-a413-4471-96fe-a51e3764c2ae",
   "metadata": {},
   "source": [
    "### f) Generate Sentences (25 points)\n",
    "\n",
    "Now that you have trained models, you'll work on the generation piece. Note that because you saved your models, even if you have to re-start your kernel, you should be able to re-load them without having to re-train them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad01dd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (embedding): Embedding(60, 50)\n",
       "  (hidden): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=128, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the models in again with code like:\n",
    "# model = FFNN(same params as when you created the model to begin with)\n",
    "# model.load_state_dict(torch.load(MODEL_FILE))\n",
    "# then switch the model into evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "word_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "word_embedder = nutils.create_embedder(word_embeddings)\n",
    "word_model = FFNN(len(word_embedder.token_to_index), NGRAM, word_embedder)\n",
    "word_model.load_state_dict(torch.load(MODEL_FILE_WORD))\n",
    "word_model.eval()\n",
    "\n",
    "char_embeddings = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)\n",
    "char_embedder = nutils.create_embedder(char_embeddings)\n",
    "char_model = FFNN(len(char_embedder.token_to_index), NGRAM, char_embedder)\n",
    "char_model.load_state_dict(torch.load(MODEL_FILE_CHAR))\n",
    "char_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "608b05fb-8db9-4d44-8cb2-bb9ec5f373d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    # YOUR CODE HERE\n",
    "    embedder = model.embedding\n",
    "    input_indices = [embedder.token_to_index[token] for token in input_tokens]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_index = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_token = embedder.index_to_token[predicted_index]\n",
    "\n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f721708d-3dde-4e91-888d-080c4ac6ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: list[str], max_tokens: int = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\"\n",
    "    generated = seed.copy()\n",
    "\n",
    "    if max_tokens is None:\n",
    "        max_tokens = 50 \n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        context = generated[-(NGRAM - 1) :]\n",
    "        next_token = predict(model, context)\n",
    "        generated.append(next_token)\n",
    "        if next_token == SENTENCE_END:\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0219cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences\n",
    "from neurallm_utils_starter import SENTENCE_BEGIN, SENTENCE_END\n",
    "\n",
    "def format_word_sentence(tokens: list[str]) -> str:\n",
    "    \"\"\"Format word tokens into readable sentence.\"\"\"\n",
    "    clean_tokens = [\n",
    "        token for token in tokens if token not in [SENTENCE_BEGIN, SENTENCE_END]\n",
    "    ]\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "\n",
    "def format_char_sentence(tokens: list[str]) -> str:\n",
    "    \"\"\"Format character tokens into readable sentence.\"\"\"\n",
    "    clean_tokens = [\n",
    "        token for token in tokens if token not in [SENTENCE_BEGIN, SENTENCE_END]\n",
    "    ]\n",
    "    sentence = \"\".join(clean_tokens)\n",
    "    return sentence.replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "def generate_multiple(model, num_sentences: int, is_char: bool = False) -> list[str]:\n",
    "    \"\"\"Generate multiple sentences from a model.\"\"\"\n",
    "    sentences = []\n",
    "    seed = [SENTENCE_BEGIN] * (NGRAM - 1)\n",
    "\n",
    "    for _ in range(num_sentences):\n",
    "        generated = generate(model, seed)\n",
    "        if is_char:\n",
    "            formatted = format_char_sentence(generated)\n",
    "        else:\n",
    "            formatted = format_word_sentence(generated)\n",
    "        sentences.append(formatted)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f46b317a-e8c7-41d4-a598-34318ffda93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WORD MODEL GENERATIONS ===\n",
      "1. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "2. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "3. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "4. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "5. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "6. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "7. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "8. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "9. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "10. i was not to be sure , and the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of the most of\n",
      "\n",
      "=== CHARACTER MODEL GENERATIONS ===\n",
      "1. i some of the of the of the of the of the of the o\n",
      "2. i some of the of the of the of the of the of the o\n",
      "3. i some of the of the of the of the of the of the o\n",
      "4. i some of the of the of the of the of the of the o\n",
      "5. i some of the of the of the of the of the of the o\n",
      "6. i some of the of the of the of the of the of the o\n",
      "7. i some of the of the of the of the of the of the o\n",
      "8. i some of the of the of the of the of the of the o\n",
      "9. i some of the of the of the of the of the of the o\n",
      "10. i some of the of the of the of the of the of the o\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# generate and display ten sequences from both your word model and your character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# For character-based, replace _ with a space\n",
    "\n",
    "print(\"=== WORD MODEL GENERATIONS ===\")\n",
    "word_sentences = generate_multiple(word_model, 10, is_char=False)\n",
    "for i, sentence in enumerate(word_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "print(\"\\n=== CHARACTER MODEL GENERATIONS ===\")\n",
    "char_sentences = generate_multiple(char_model, 10, is_char=True)\n",
    "for i, sentence in enumerate(char_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "703a1ec4-9a8b-46dc-894d-b41be323058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 sentences for each model...\n",
      "Saved 100 word sentences to generated_wordbased.txt\n",
      "Saved 100 char sentences to generated_charbased.txt\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Generate 100 example sentences with each model and save them to two files, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "# We've defined the filenames for you at the top of this notebook\n",
    "# Do not print these sentences here :)\n",
    "\n",
    "print(\"Generating 100 sentences for each model...\")\n",
    "word_sentences_100 = generate_multiple(word_model, 100, is_char=False)\n",
    "with open(OUTPUT_WORDS, \"w\") as f:\n",
    "    for sentence in word_sentences_100:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "char_sentences_100 = generate_multiple(char_model, 100, is_char=True)\n",
    "with open(OUTPUT_CHARS, \"w\") as f:\n",
    "    for sentence in char_sentences_100:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "print(f\"Saved 100 word sentences to {OUTPUT_WORDS}\")\n",
    "print(f\"Saved 100 char sentences to {OUTPUT_CHARS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef7860",
   "metadata": {},
   "source": [
    "11. What were the final parameters that you used for your model? \n",
    "- Word based?\n",
    "- N: __3__\n",
    "- embedding size: __50__\n",
    "- epochs: __3__\n",
    "- hidden units: __256__\n",
    "- learning rate: __0.001__\n",
    "- training time + system you were running it on (operating system + chip/specs): __6-7min total (3 epochs 2 min each) on macOS with M1 chip__\n",
    "    - for pairs, you can either note both partners' training times or just one\n",
    "\n",
    "- What was the word-based model's final loss? __5.1393__\n",
    "- Character based? \n",
    "- N: __3__\n",
    "- embedding size: __50__\n",
    "- epochs: __5__\n",
    "- hidden units: __128__\n",
    "- learning rate: __0.001__\n",
    "- training time + system you were running it on (operating system + chip/specs): __1.5-2min total (5 epochs 20 seconds each) on macOS with M1 chip__\n",
    "    - for pairs, you can either note both partners' training times or just one\n",
    "\n",
    "- What was the word-based model's final loss? __1.9567__\n",
    "\n",
    "If you used different parameters for your word-based and character-based models, note the different parameters clearly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv311)",
   "language": "python",
   "name": "myenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
