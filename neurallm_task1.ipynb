{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5313d72c-9835-45fb-bfcb-90cf977062b6",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data)\n",
    "----\n",
    "\n",
    "Due date: Tuesday, June 3rd, 2025 @ 9pm Boston time\n",
    "\n",
    "Points: 132.5 (see Canvas for breakdown)\n",
    "\n",
    "Goals:\n",
    "- explore & use word embeddings\n",
    "- train neural models from the ground up!\n",
    "- get comfy with a modern neural net library (`pytorch`)\n",
    "    - you'll likelye make close friends with the docs: https://pytorch.org/tutorials/beginner/basics/intro.html \n",
    "- evaluate neural vs. vanilla n-gram language models\n",
    "\n",
    "Complete in groups of: __two__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project!\n",
    "\n",
    "Allowed python modules:\n",
    "- `gensim`, `numpy`, `matplotlib`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, all built-in python libraries (e.g. `math` and `string`), and anything else we imported in the starter code\n",
    "- if you would like to use a library not on this list, post on piazza to request permission\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
    "\n",
    "Data processing:\n",
    "- You may __choose__ how you would like to tokenize your text for this assignment\n",
    "- You'll want to __deal with internal commas (commas inside of the sentences) appropriately__ when you read in the data, so use the python [`csv` module](https://docs.python.org/3/library/csv.html) or some other module to read the csv in (vs. splitting on commas).\n",
    "\n",
    "Warnings:\n",
    "- You might see:\n",
    "```\n",
    "notebook controller is DISPOSED. \n",
    "View Jupyter log for further details.\n",
    "```\n",
    "This is not an error per se--go to the last cell that ran successfully (or the first cell) and run them one-by-one, waiting for the prior one to finish running before moving to the next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5940a9b-f44d-49df-b0da-0170ffc6b41d",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: __William Aoun__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7228d8-3ab8-4038-9f49-c4e58938fde5",
   "metadata": {},
   "source": [
    "Task 1: Provided Data Write-Up (7 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e3e2a-4db8-4554-a487-267358be5a6f",
   "metadata": {},
   "source": [
    "This is about the __provided__ ðŸŽƒ spooky ðŸ‘» authors ðŸ§Ÿ data set. Please __bold__ your answers to all written questions! Each row in this dataset represents one sentence.\n",
    "\n",
    "1. Where did you get the data from? The provided dataset is the training data from: https://www.kaggle.com/competitions/spooky-author-identification\n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)? __Doesn't specify exactly how the data was retrieved, only that they're excerpts from horror stories by Edgar Allen Poe, Mary Shelley, and HP Lovecraft.__\n",
    "3. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc) __Excerpts from horror stories by Edgar Allen Poe, Mary Shelley, and HP Lovecraft.__\n",
    "4. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people) __The original texts were written by Edgar Allen Poe, Mary Shelley, and HP Lovecraft.__\n",
    "5. (1 pt) How large is the dataset? (# texts/sentences, # total tokens by word) __19579 sentences/texts, 594922 words__\n",
    "6. (1 pt) What are the minimum, maximum, and average sentence lengths (by tokens) in your dataset? __min 4, max 876, avg 30.4__\n",
    "7. (2 pts) How large is the vocabulary, both tokenized by character and by word? __vocab by word is 25374, vocab by char is 60__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f5d3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/billyaoun/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc22bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils_starter as nutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c30f774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "words: 594922\n",
      "avg: 30.385719393227436\n",
      "min: 4\n",
      "max: 876\n",
      "word vocab: 25374\n",
      "char vovab: 60\n"
     ]
    }
   ],
   "source": [
    "# code that you need to answer the above questions here!\n",
    "# but make sure that you leave the answers you want us to grade in the markdown!\n",
    "df = pd.read_csv(\"spooky_author_train.csv\")\n",
    "print(len(df))\n",
    "\n",
    "ngram = 1\n",
    "tokenized_data = nutils.read_file_spooky(\n",
    "    \"spooky_author_train.csv\", ngram=ngram, by_character=False\n",
    ")\n",
    "\n",
    "if ngram == 1:\n",
    "    token_counts = [len(sentence) - 2 for sentence in tokenized_data]\n",
    "else:\n",
    "    boundary_tokens = 2 * (ngram - 1)\n",
    "    token_counts = [len(sentence) - boundary_tokens for sentence in tokenized_data]\n",
    "\n",
    "total_tokens = sum(token_counts)\n",
    "\n",
    "print(f\"words: {total_tokens}\")\n",
    "print(f\"avg: {sum(token_counts)/len(token_counts)}\")\n",
    "print(f\"min: {min(token_counts)}\")\n",
    "print(f\"max: {max(token_counts)}\")\n",
    "\n",
    "word_tokenized_data = nutils.read_file_spooky(\n",
    "    \"spooky_author_train.csv\", ngram=1, by_character=False\n",
    ")\n",
    "\n",
    "char_tokenized_data = nutils.read_file_spooky(\n",
    "    \"spooky_author_train.csv\", ngram=1, by_character=True\n",
    ")\n",
    "\n",
    "word_vocab = set()\n",
    "for sentence in word_tokenized_data:\n",
    "    word_vocab.update(sentence)\n",
    "\n",
    "char_vocab = set()\n",
    "for sentence in char_tokenized_data:\n",
    "    char_vocab.update(sentence)\n",
    "\n",
    "print(f\"word vocab: {len(word_vocab)}\")\n",
    "print(f\"char vovab: {len(char_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e2e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv311)",
   "language": "python",
   "name": "myenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
